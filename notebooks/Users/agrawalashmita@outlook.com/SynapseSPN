# Databricks notebook source
spark.conf.set("fs.azure.account.auth.type", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type",  "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set("fs.azure.account.oauth2.client.id", "72d1bd13-33b4-40fa-9d43-4a1a586e0a43")
spark.conf.set("fs.azure.account.oauth2.client.secret", "8TvdIpQU4__CqjeU.i.6dtPX54rqHR7x4u")
spark.conf.set("fs.azure.account.oauth2.client.endpoint", "https://login.microsoftonline.com/3ed8fb36-44b8-4d07-a35d-360a8ee420e7/oauth2/token")

# Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)
spark.conf.set("spark.databricks.sqldw.jdbc.service.principal.client.id", "72d1bd13-33b4-40fa-9d43-4a1a586e0a43")
spark.conf.set("spark.databricks.sqldw.jdbc.service.principal.client.secret", "8TvdIpQU4__CqjeU.i.6dtPX54rqHR7x4u")

# COMMAND ----------

spark.conf.set(
  "fs.azure.account.key.pocteat.blob.core.windows.net",
  "KOrSeyfFMW3XFXfbcUGIYCkH+Dmu5EFEuGAkkdM8bRaHGthedex03gzWRwmRklLYa1fmHPMWu0xH2Cqjeyi+jQ==")



# COMMAND ----------

df = spark.read \
   .format("com.databricks.spark.sqldw") \
   .option("url", "jdbc:sqlserver://ashpoc.database.windows.net:1433;database=POCTRIAL;encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.sql.azuresynapse.net;Authentication=ActiveDirectoryServicePrincipal;loginTimeout=30") \
   .option("tempDir", "abfss://poc@pocteat.dfs.core.windows.net/") \
   .option("enableServicePrincipalAuth", "true") \
   .option("query", "select * from dbo.DimDate") \
   .load()

# COMMAND ----------

df.printSchema()

# COMMAND ----------

df.show()